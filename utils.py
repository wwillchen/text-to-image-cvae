import torchvision.utils as vutils
from matplotlib import pyplot as plt
import torch
import torchvision.transforms as transforms
from models.baseline_cvae import BaselineConvVAE
from PIL import Image
import os

def sample_and_save(vae, captions, epoch, device, output_dir="samples"):
    """
    Samples images from the BaselineConvVAE and saves them.
    
    Args:
        vae: The trained BaselineConvVAE model.
        captions: A batch of caption embeddings from the dataset.
        epoch: Current epoch number.
        device: Device (CPU or GPU) on which the model runs.
        output_dir: Directory to save the sampled images.
    """
    os.makedirs(output_dir, exist_ok=True)
    
    batch_size = captions.size(0)
    latent_dim = vae.latent_dim
    
    z = torch.randn(batch_size, latent_dim).to(device)

    random_caption_idx = torch.randint(0, captions.size(0), (1,)).item()
    selected_caption = captions[random_caption_idx].unsqueeze(0).repeat(batch_size, 1).to(device)

    vae.eval()
    with torch.no_grad():
        generated_images = vae.decode(z, selected_caption)
        
    generated_images = (generated_images.clamp(0, 1) * 255).to(torch.uint8)

    image_path = os.path.join(output_dir, f"sample_epoch_{epoch}.png")
    pil_image = transforms.ToPILImage()(generated_images[0]) 
    pil_image.save(image_path)  

def sample_from_decoder(vae, annotations, bert, tokenizer, num_samples=5, device='cuda'):
    """
    Sample multiple images from the VAE decoder given a set of distinct annotations (captions).
    Each annotation is processed, and multiple latent samples are generated and decoded.

    Args:
        vae: The trained VAE model.
        annotations: A list of distinct caption strings.
        bert: Pre-trained BERT model for encoding captions.
        tokenizer: Tokenizer for the BERT model.
        num_samples: Number of random latent samples to generate per caption.
        device: The device to use ('cuda' or 'cpu').

    Returns:
        A list of tuples [(generated_image, caption), ...], where:
            generated_image: A PIL Image generated by the decoder.
            caption: The corresponding caption string.
    """
    vae.eval()
    bert.to(device)

    generated_results = []

    for caption in annotations:
        # Encode the caption using BERT
        inputs = tokenizer(caption, return_tensors='pt', add_special_tokens=True)
        inputs = {key: val.to(device) for key, val in inputs.items()}
        with torch.no_grad():
            outputs = bert(**inputs)
            last_hidden_state = outputs.last_hidden_state

        token_embeddings = last_hidden_state[0]  
        attention_mask = inputs['attention_mask'][0] 

        masked_token_embeddings = token_embeddings * attention_mask.unsqueeze(-1)
        caption_encoding = masked_token_embeddings.sum(dim=0) / attention_mask.sum()
        caption_encoding = caption_encoding.unsqueeze(0)  

        with torch.no_grad():
            for _ in range(num_samples):
                z_dim = vae.latent_dim if hasattr(vae, 'latent_dim') else 128
                z = torch.randn((1, z_dim)).to(device)

                decoder_output = vae.decode(z, caption_encoding)

                reconstructed_image = decoder_output.squeeze(0)  # [C, H, W]
                reconstructed_image = (reconstructed_image.clamp(0, 1) * 255).to(torch.uint8)
                pil_image = transforms.ToPILImage()(reconstructed_image.cpu())

                generated_results.append((pil_image, caption))

    return generated_results

def load_model(model_path, device='cuda', latent_dim=128, text_embedding_dim=768):
    model = BaselineConvVAE(text_embedding_dim=text_embedding_dim, latent_dim=latent_dim)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()
    return model

def create_caption_grid(images, caption, output_path):
    num_images = len(images)

    fig, axes = plt.subplots(1, num_images, figsize=(num_images * 3, 4))
    if num_images == 1:
        axes = [axes]  

    for ax, img in zip(axes, images):
        ax.imshow(img)
        ax.axis("off")

    plt.suptitle(caption, y=0.1, fontsize=12)
    plt.tight_layout()

    plt.savefig(output_path, bbox_inches="tight")
    plt.close(fig)